1. Web Scraping : A web scraper can be used to extract the text content from the website, including articles, product descriptions, FAQs, etc. 
This extracted text becomes the training data for the model.

Techniques for web scraping and crawling:
- BeautifulSoup Python Scraping Library: Beautiful Soup provides methods to search and navigate through the html content. You can find elements
  by tag name, attribute, or text content. You can target specific elements like product listings, news articles, or reviews.
- Scrapy- Python Web Scraping and crawling framework
- Selenium Web Scraping Python Library

--> For scraping simple websites or those with static content, Beautiful Soup or Scrapy might be sufficient.
--> If you need to scrape data from websites that heavily rely on JavaScript or require complex interactions, Selenium becomes 
    a valuable tool.

challenges of web scraping:
- Website Changes: Websites frequently update their structure and design, which can break your scraper if it relies on specific elements.
- Complex Sites: Sites with heavy use of Javascript or dynamic content can be difficult for scrapers to handle.
- Choosing a Scraping Method:
  Simple Scraping: If the website structure is relatively static and doesn't heavily rely on JavaScript, basic web scraping tools 
  or libraries (BeautifulSoup in Python) might suffice.
  JavaScript Rendering: For websites with dynamic content generated by JavaScript, consider using headless browsers 
  (like Selenium) to simulate a real browser and render the complete content before scraping.
- Handling Anti-Scraping Measures:
  Respectful Scraping: Implement techniques like scraping at a moderate pace, rotating IP addresses (using proxies), and following 
  robots.txt guidelines to avoid triggering anti-scraping mechanisms.
  Alternative Approaches: If scraping becomes difficult due to aggressive anti-scraping measures, consider contacting the website 
  owners and exploring alternative data access methods (APIs) if available.
- Scalability: If you plan to scrape large websites or scrape frequently, consider the scalability of your scraping and processing 
  infrastructure.

2. Scrapy- Python Web Scraping and crawling framework
- Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data 
  from their pages.
- Main advantages about Scrapy is, Scrapy doesnâ€™t need to wait for a request to be finished and processed, it can send another
  request or do other things in the meantime. This also means that other requests can keep going even if some request fails
  or an error happens while handling it.

Crawling an entire website using a single URL involves a  recursive  approach.
- Initialize an empty set to store visited URLs (to avoid revisiting the same page).
- Use a library like requests in Python to download the HTML content of the seed URL.
- Use Beautiful Soup to parse the downloaded HTML content, creating a tree structure for easy navigation.
- Find all anchor tags (<a>) within the parsed HTML tree using Beautiful Soup's search methods.
- Extract the URLs from the href attribute of these anchor tags.
- These urls should be used for scraping.

Technical Limitations:
- Dynamic Content: Scrapy struggles with websites that heavily rely on JavaScript for content rendering. While workarounds
  like integrating with Selenium exist, they add complexity and overhead.
- Complex Captchas: Advanced CAPTCHAs can be difficult for Scrapy to bypass automatically. Manual intervention or 
  specialized services might be needed.
- Infinite Scrolling: Websites using infinite scrolling techniques for content loading may require additional logic 
  in your Scrapy spider to handle these scenarios effectively.
- Blocking Mechanisms: Websites can detect and block scrapers if they see excessive requests coming from a single IP address.
  Utilizing a pool of IP addresses or rotating User-Agents can help mitigate this risk.This can be achieved by using various
  services that offer proxy networks with large pools of IP addresses. These services allow your scraper to send requests through 
  different IP addresses with each request, making it appear like traffic is coming from multiple users rather than a single source.


3. Selenium: When dealing with heavily JavaScript-driven websites, Selenium offers browser automation capabilities that 
  Scrapy lacks. However, Selenium can be slower and more resource-intensive.
- Dynamic Content: Unlike tools like Beautiful Soup, Selenium can handle websites that heavily rely on JavaScript for 
  content rendering. It allows you to interact with the browser and wait for the content to load dynamically before 
  scraping.
- Complex Interactions: Selenium can simulate complex user interactions like scrolling through pages, clicking elements
  that load new content, or filling out forms with dynamic elements. This enables scraping data that might not be readily
  available in the initial HTML source.
- CAPTCHA Handling: While not foolproof, Selenium can integrate with services or tools that specialize in solving CAPTCHAs,
  potentially bypassing them during scraping.


4. Text Preprocessing : The scraped text needs cleaning to remove irrelevant information like HTML tags, advertisements, 
and navigation menus. Techniques like tokenization (breaking text into words) and stemming/lemmatization (reducing words to their 
root form) might be applied.

challenges of Text Preprocessing:
- Incomplete Data: The scraped data might not include everything you need, or it could be missing crucial context.
- Inconsistent Data Structure: Websites often present information in various formats. Articles might have different structures, 
  product descriptions might lack consistency, and user reviews might have varying levels of detail. This inconsistency can make 
  it difficult to develop a single preprocessing pipeline that works for all scraped data.
- Unstructured Data: Websites often contain a mix of structured data (tables, lists) and unstructured data (text paragraphs). 
  Extracting and organizing the relevant information from both types of data can require different techniques.

Here are some strategies to tackle these preprocessing challenges:
- Data Classification: Categorize your scraped data into different types (articles, products, reviews) and develop targeted 
  preprocessing pipelines for each category.
- Data Extraction: Develop methods to extract specific information from structured and unstructured data. This might involve 
  using HTML parsing libraries or regular expressions to target relevant elements.
- Normalization: Standardize the format of your data. This can involve converting dates to a common format, ensuring consistent 
  capitalization, or stemming/lemmatization of words.
- Validation: Implement checks to ensure the quality and consistency of your preprocessed data